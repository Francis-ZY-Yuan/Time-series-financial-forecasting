{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aPQT-9Byhu9w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import listdir\n",
        "import seaborn as sns\n",
        "import functools as ft\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_folder = '/content/drive/MyDrive/DS_Capstone/data/nasdaq/quarterly/income_statement'\n",
        "bs_folder = '/content/drive/MyDrive/DS_Capstone/data/nasdaq/quarterly/balance_sheet'\n",
        "cf_folder = '/content/drive/MyDrive/DS_Capstone/data/nasdaq/quarterly/cash_flow'\n",
        "macro_file = '/content/drive/MyDrive/DS_Capstone/data/macro/macro.csv'\n",
        "dfs = {\n",
        "    'is': [pd.read_csv(is_folder + '/' + csv) for csv in listdir(is_folder)],\n",
        "    'bs': [pd.read_csv(bs_folder + '/' + csv) for csv in listdir(bs_folder)],\n",
        "    'cf': [pd.read_csv(cf_folder + '/' + csv) for csv in listdir(cf_folder)]\n",
        "       }\n",
        "df_all_by_type = {'is': pd.concat(dfs['is']), 'bs': pd.concat(dfs['bs']), 'cf': pd.concat(dfs['cf'])}\n",
        "df_all = ft.reduce(lambda left, right: pd.merge(left, right, on=['symbol', 'date']), list(df_all_by_type.values()))\n",
        "\n",
        "df_macro = pd.read_csv(macro_file)"
      ],
      "metadata": {
        "id": "bhNa-4juicjn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['date'].astype('datetime64[ns]')\n",
        "df_all.sort_values(by=['symbol', 'date'], inplace=True)\n",
        "df_all = df_all[df_all['symbol'].map(df_all['symbol'].value_counts()) >= 20]\n",
        "df_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG0V6aXnOE3B",
        "outputId": "4911228c-c51e-4722-d693-211709540b65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(171560, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "AdbRHey1YyGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out features containing lots of NAs based on cutoff\n",
        "\n",
        "cutoff = 0.05\n",
        "col_na = np.mean(df_all.select_dtypes(include=['float64']).isna())\n",
        "f_left = col_na[col_na <= cutoff].index.to_list()\n",
        "f_left.remove('netIncome_y')\n",
        "\n",
        "# find and delete symbols contributing NAs remains\n",
        "\n",
        "df_tmp = df_all[['date', 'symbol'] + f_left]\n",
        "\n",
        "symbols_to_drop = df_tmp[df_tmp.isnull().any(axis=1)]['symbol'].unique()\n",
        "\n",
        "df_raw = df_tmp[~df_tmp['symbol'].isin(symbols_to_drop)]\n",
        "df_raw.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDUOpl2gPIhB",
        "outputId": "0c78df1b-3270-42aa-de26-91f7e381b488"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16368, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check multicollinearity\n",
        "\n",
        "sns.heatmap(df_raw[f_left].corr(), cmap='GnBu', annot=True)\n",
        "plt.rcParams['figure.figsize'] = (30, 10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bB7P3MVvRkTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "look_back = 6\n",
        "targets = [\n",
        " 'ebit',\n",
        " 'totalOperatingExpenses',\n",
        " 'totalLiab',\n",
        " 'totalStockholderEquity',\n",
        " 'commonStockSharesOutstanding',\n",
        " 'otherCashflowsFromFinancingActivities',\n",
        " 'capitalExpenditures']\n",
        "\n",
        "features = targets + ['GDP', 'UNEMP', 'CPI', 'PPI', 'INDO']\n",
        "df_raw_mac = df_raw.merge(df_macro, on=['date'], how='left')\n",
        "# df_raw = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "7snaVm6Oi_XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Analysis"
      ],
      "metadata": {
        "id": "0wrTMUHvfBmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw_scaled = pd.DataFrame(np.hstack((np.array(df_raw[['date', 'symbol']]), \n",
        "                   np.vstack([MinMaxScaler().fit_transform(y.drop(columns=['symbol', 'date'])) for _, y in df_raw.groupby('symbol')])\n",
        "                   )), columns=df_raw.columns)\n",
        "df_raw_scaled['quarter'] = df_raw_scaled.date.str[-5:-3].astype(int)%12 // 3 + 1\n",
        "\n",
        "fig, axes = plt.subplots(len(targets), figsize=(10, 15), sharex=True)\n",
        "for i, f in enumerate(targets): sns.boxplot(data=df_raw_scaled, x='quarter', y=f, ax=axes[i])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nWref11TRmjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils functions"
      ],
      "metadata": {
        "id": "TkCeSuC2Pb_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy.lib.stride_tricks import sliding_window_view\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# convert sequences into samples\n",
        "def df2samples(df, look_back, features, targets):\n",
        "  train_X, train_Y, test_X, test_Y = [], [], [], []\n",
        "  for _, df_symbol in df.groupby('symbol'):\n",
        "    \n",
        "    df_symbol = df_symbol.drop(columns=['symbol', 'date'])\n",
        "    \n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit_transform(df_symbol.iloc[:-2])\n",
        "    df_scaled = pd.DataFrame(scaler.transform(df_symbol), columns=df_symbol.columns)\n",
        "    \n",
        "    all_Y = df_scaled[targets].to_numpy()[look_back:]\n",
        "    all_X = sliding_window_view(df_scaled[features].to_numpy()[:-1], (look_back, len(features))).squeeze()\n",
        "\n",
        "    train_X.append(all_X[:-1]), test_X.append(all_X[-1].reshape((1, look_back, len(features))))\n",
        "    train_Y.append(all_Y[:-1]), test_Y.append(all_Y[-1])\n",
        "  return np.vstack(train_X), np.vstack(train_Y), np.vstack(test_X), np.vstack(test_Y)\n",
        "\n",
        "\n",
        "# plot grid search result\n",
        "def gs_heatmap(values, x, y, x_label='LSTM units', y_label='look_back'):\n",
        "  values = np.array(values).reshape((len(y), len(x)))\n",
        "  fig = sns.heatmap(values, cmap='rocket_r', square=True, linewidth=.2, xticklabels=x, yticklabels=y)\n",
        "  fig.set_ylabel(y_label)\n",
        "  fig.set_xlabel(x_label)\n",
        "  fig.set_title('Loss by grid search')\n",
        "  cbar = fig.collections[0].colorbar\n",
        "  cbar.set_ticks([cbar.get_ticks()[-1], cbar.get_ticks()[0]])\n",
        "  cbar.set_ticklabels(['High', 'Low'])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# smape for evaluation\n",
        "def SMAPE(y_pred, y_act): \n",
        "    n = np.abs((y_act - y_pred))\n",
        "    d = np.abs(y_act) + np.abs(y_pred)\n",
        "    return 200 * np.mean(np.where(d == 0, 0, n / d), 0)\n",
        "\n",
        "# smape for validation\n",
        "def smape(y_true, y_pred):\n",
        "    import keras.backend as K\n",
        "    return 200 * K.mean(K.abs(y_pred - y_true) / ((K.abs(y_true) + K.abs(y_pred))), axis=-1)"
      ],
      "metadata": {
        "id": "35YM82qWPgxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA - Baseline"
      ],
      "metadata": {
        "id": "daWRPrRIBjVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima -q\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "\n",
        "def ARIMA(df, features):\n",
        "    \n",
        "    test_Y, pred_Y = [], []\n",
        "    for _, df_symbol in df.groupby('symbol'):\n",
        "        df_symbol = df_symbol[features]\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit_transform(df_symbol.iloc[:-1])\n",
        "        all_X = scaler.transform(df_symbol)\n",
        "        pred_Y.append([auto_arima(all_X[:-1, i]).predict(n_periods=1)[0] for i in range(len(features))])\n",
        "        test_Y.append(all_X[-1])\n",
        "\n",
        "        \n",
        "    pred_Y, test_Y = np.stack(pred_Y), np.stack(test_Y)\n",
        "    return SMAPE(pred_Y, test_Y)\n",
        "\n",
        "ARIMA(df_raw, targets)"
      ],
      "metadata": {
        "id": "RM-siRmdBhvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate LSTM"
      ],
      "metadata": {
        "id": "ndrGma7dO-AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, Callback\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "\n",
        "class LSTM_m2m(object):\n",
        "\n",
        "    def __init__(self, units, look_back, n_features, n_targets):\n",
        "\n",
        "\n",
        "        # Get model hyperparameters\n",
        "        self.units = units\n",
        "        self.look_back = look_back\n",
        "        self.n_features = n_features\n",
        "        self.n_targets = n_targets\n",
        "\n",
        "        # Get directories name\n",
        "        self.log_dir = 'log'\n",
        "        self.checkpoint_dir = 'checkpoint'\n",
        "        self.dropout = 0.25\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(self.units, input_shape=(self.look_back, self.n_features)))\n",
        "        # model.add(LSTM(units, input_shape=(self.look_back, self.n_features), return_sequences=True))\n",
        "        # model.add(LSTM(64))\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "        model.add(Dense(self.n_targets))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    def restore(self, filepath):\n",
        "\n",
        "        # Load the architecture\n",
        "        self.best_model = load_model(filepath, custom_objects={'smape': smape})\n",
        "        self.best_model.compile(optimizer='adam', loss=['mse'], metrics=[smape])\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=200, batch_size=64, verbose=1):\n",
        "\n",
        "        self.model = self.build()\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=['mse'], metrics=[smape])\n",
        "\n",
        "        # Stop training if error does not improve within 25 iterations\n",
        "        early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "        # Save the best model ... with minimal error\n",
        "        filepath = self.checkpoint_dir+'/Transformer.best'+datetime.now().strftime('%d%m%Y_%H:%M:%S')+'.hdf5'\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=verbose, save_best_only=True, mode='min')\n",
        "\n",
        "        callback_history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                             validation_split=0.2, verbose=verbose,\n",
        "                             callbacks=[early_stopping_monitor, checkpoint])\n",
        "        return callback_history\n",
        "\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        loss, _ = self.model.evaluate(X_test, y_test, verbose=0) \n",
        "\n",
        "        return loss, SMAPE(y_pred, y_test)"
      ],
      "metadata": {
        "id": "UmtIkOB-QVnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM trained on targts\n",
        "\n",
        "lstm_result = []\n",
        "\n",
        "for look_back in range(4, 17, 2):\n",
        "  train_X, train_Y, test_X, test_Y = df2samples(df_raw, look_back, targets, targets)\n",
        "  for units in np.linspace(64, 128, 6, dtype=int):\n",
        "    lstm = LSTM_m2m(units, look_back, len(targets), len(targets))\n",
        "    lstm.train(train_X, train_Y, batch_size=256, verbose=0)\n",
        "    lstm_result.append(lstm.evaluate(test_X, test_Y))\n",
        "\n",
        "gs_loss = [result[0] for result in lstm_result]\n",
        "gs_heatmap(gs_loss, x=np.linspace(64, 128, 6, dtype=int), y=range(4, 17, 2))\n",
        "print('optimal LSTM on test: ', lstm_result[np.argmin(gs_loss)][1])"
      ],
      "metadata": {
        "id": "SHGhxhCURsQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM trained on features including macro indicators\n",
        "\n",
        "lstm_m_result = []\n",
        "\n",
        "for look_back in range(4, 17, 2):\n",
        "  train_X, train_Y, test_X, test_Y = df2samples(df_raw_mac, look_back, features, targets)\n",
        "  for units in np.linspace(64, 128, 6, dtype=int):\n",
        "    lstm = LSTM_m2m(units, look_back, len(features), len(targets))\n",
        "    lstm.train(train_X, train_Y, batch_size=256, verbose=0)\n",
        "    lstm_m_result.append(lstm.evaluate(test_X, test_Y))\n",
        "\n",
        "gs_m_loss = [result[0] for result in lstm_m_result]\n",
        "gs_heatmap(gs_m_loss, x=np.linspace(64, 128, 6, dtype=int), y=range(4, 17, 2))\n",
        "print('optimal LSTM with macro on test: ', lstm_m_result[np.argmin(gs_m_loss)][1])"
      ],
      "metadata": {
        "id": "dQP8jwT0RuIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "MqnBfaH1QoFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade -q\n",
        "\n",
        "\n",
        "from keras import Input, Model\n",
        "from keras.layers import LayerNormalization, MultiHeadAttention, Dropout, Conv1D, GlobalAveragePooling1D, Dense\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, Callback\n",
        "\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "\n",
        "class Transformer(object):\n",
        "\n",
        "    def __init__(self, look_back, n_features, n_targets):\n",
        "\n",
        "        with open('parameters.json') as f:\n",
        "            parameters = json.load(f)\n",
        "\n",
        "\n",
        "        # Get model hyperparameters\n",
        "        self.look_back = look_back\n",
        "        self.n_features = n_features\n",
        "        self.n_targets = n_targets\n",
        "        # Get directories name\n",
        "        self.log_dir = 'log'\n",
        "        self.checkpoint_dir = 'checkpoint'\n",
        "\n",
        "        self.head_size=256\n",
        "        self.num_heads=10\n",
        "        self.ff_dim=4\n",
        "        self.num_transformer_blocks=6\n",
        "        self.mlp_units=[128]\n",
        "        self.mlp_dropout=0.1\n",
        "        self.dropout=0.1\n",
        "\n",
        "\n",
        "    def transformer_encoder(self, inputs):\n",
        "\n",
        "        # Normalization and Attention\n",
        "        x = LayerNormalization(epsilon=1e-6)(inputs)\n",
        "        x = MultiHeadAttention(key_dim=self.head_size, num_heads=self.num_heads, dropout=self.dropout)(x, x)\n",
        "        x = Dropout(self.dropout)(x)\n",
        "\n",
        "        res = x + inputs\n",
        "\n",
        "        # Feed Forward Part\n",
        "        x = LayerNormalization(epsilon=1e-6)(res)\n",
        "        x = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')(x)\n",
        "        x = Dropout(self.dropout)(x)\n",
        "        x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "        return x + res\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "\n",
        "        inputs = Input(shape=(self.look_back, self.n_features))\n",
        "        x = inputs\n",
        "        for _ in range(self.num_transformer_blocks): x = self.transformer_encoder(x)\n",
        "\n",
        "        x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "        for dim in self.mlp_units:\n",
        "            x = Dense(dim, activation='relu')(x)\n",
        "            x = Dropout(self.mlp_dropout)(x)\n",
        "\n",
        "        # output layer\n",
        "        outputs = Dense(self.n_targets)(x)\n",
        "\n",
        "        return Model(inputs, outputs)\n",
        "\n",
        "    def restore(self, filepath):\n",
        "\n",
        "        # Load the architecture\n",
        "        self.best_model = load_model(filepath, custom_objects={'smape': smape})\n",
        "        self.best_model.compile(optimizer='adam', loss=['mse'], metrics=[smape])\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=200, batch_size=256, verbose=1):\n",
        "\n",
        "        self.model = self.build()\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=['mse'], metrics=[smape])\n",
        "\n",
        "\n",
        "        # Stop training if error does not improve within 25 iterations\n",
        "        early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
        "\n",
        "        # Save the best model ... with minimal error\n",
        "        filepath = self.checkpoint_dir+'/Transformer.best'+datetime.now().strftime('%d%m%Y_%H:%M:%S')+'.hdf5'\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=verbose, save_best_only=True, mode='min')\n",
        "\n",
        "        callback_history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
        "                             validation_split=0.2, verbose=verbose,\n",
        "                             callbacks=[early_stopping_monitor, checkpoint])\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        loss, _ = self.model.evaluate(X_test, y_test, verbose=0) \n",
        "\n",
        "        return loss, SMAPE(y_pred, y_test)"
      ],
      "metadata": {
        "id": "6301YLntQkR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer trained on targts\n",
        "tr_result = []\n",
        "\n",
        "for look_back in range(4, 17, 2):\n",
        "  train_X, train_Y, test_X, test_Y = df2samples(df_raw, look_back, targets, targets)\n",
        "  tr = Transformer(look_back, len(targets), len(targets))\n",
        "  tr.train(train_X, train_Y, verbose=0)\n",
        "  tr_result.append(tr.evaluate(test_X, test_Y))\n",
        "\n",
        "gs_loss = [result[0] for result in tr_result]\n",
        "print('optimal Transformer on test: ', tr_result[np.argmin(gs_loss)][1])\n",
        "    \n",
        "plt.plot(range(4, 17, 2), gs_loss)\n",
        "plt.xlabel('look_back')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Transformer Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXBz8dp2RwQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer trained on features including macro indicators\n",
        "\n",
        "tr_m_result = []\n",
        "\n",
        "for look_back in range(4, 17, 2):\n",
        "  train_X, train_Y, test_X, test_Y = df2samples(df_raw, look_back, features, targets)\n",
        "  tr = Transformer(look_back, len(features), len(targets))\n",
        "  tr.train(train_X, train_Y, verbose=0)\n",
        "  tr_m_result.append(tr.evaluate(test_X, test_Y))\n",
        "\n",
        "gs_loss = [result[0] for result in tr_m_result]\n",
        "print('optimal Transformer with macro on test: ', tr_m_result[np.argmin(gs_loss)][1])\n",
        "    \n",
        "plt.plot(range(4, 17, 2), gs_loss)\n",
        "plt.xlabel('look_back')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Transformer Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g0LVPPJ2Qk1q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}